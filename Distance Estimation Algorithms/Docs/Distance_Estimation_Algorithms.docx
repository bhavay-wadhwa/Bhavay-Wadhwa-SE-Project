Title: Distance Estimation Algorithms for Real-Time Pedestrian & Vehicle Detection

--- 
Introduction
---
Distance estimation is an important component of real-time detection systems (pedestrians & vehicles). It helps with alerts, proximity warnings, collision avoidance, and analytics. The following document lists candidate algorithms ordered by practicality for a real-time system prioritizing speed, robustness in low light/clutter, and implementation complexity.

Summary Table (quick view)
---
1. Pinhole Camera Model — Fast, accurate if camera calibrated, minimal compute
2. Bounding-Box Pixel-Height Method — Very fast, minimal calibration, limited for varied object heights & camera tilt
3. Stereo Vision (Disparity) — High accuracy, requires stereo rig and more compute
4. Monocular Depth (MiDaS or similar) — Works with single camera, robust in some low-light cases if network trained for it, heavier compute

--- 
Algorithm 1: Pinhole Camera Model
---
Description
The pinhole model uses geometry to estimate distance:
  distance = (real_height * focal_length) / pixel_height

Requirements
- Known real-world object height (e.g., average human height ~1.7 m, vehicle heights known per class)
- Camera focal length in pixels (f_x or computed from calibration)
- Camera must be roughly level, or you must account for pitch & mounting height

Pros
- Very fast (constant-time algebraic computation)
- Accurate when camera calibration and object heights are correct

Cons
- Requires calibration and reasonably consistent object height
- Sensitive to object pose and camera tilt

Implementation notes
- Calibrate camera using chessboard or OpenCV calibration routines to get focal length (f_x) in pixels.
- Use bounding-box height as pixel_height for the detected object.

--- 
Algorithm 2: Bounding Box Pixel-Height Method (heuristic)
---
Description
Use empirical mapping from bounding box height (or bottom y coordinate) to distance. Build a lookup table or regression model:
  distance ≈ a / pixel_height + b
or use piecewise linear mapping from labeled sample frames.

Requirements
- Some labeled examples or known mounting geometry
- No explicit camera calibration required, but better with one

Pros
- Extremely fast and simple to implement
- No deep models; robust enough for many front-facing cameras

Cons
- Needs dataset-specific tuning
- Less accurate across camera setups and object height variance

Implementation notes
- Collect pairs (pixel_height, true_distance) and fit a curve (e.g., inverse linear).
- Use class-specific models (person vs. car) for better accuracy.

--- 
Algorithm 3: Stereo Vision (depth from disparity)
---
Description
Stereo systems compute disparity between left/right images and compute depth:
  depth = (baseline * focal_length) / disparity

Requirements
- Two calibrated cameras with known baseline and rectified images
- Stereo matcher (OpenCV StereoBM / StereoSGBM or deep stereo net)

Pros
- High accuracy with proper calibration and rectification
- Works over a range of distances

Cons
- Extra hardware (stereo rig)
- Computational cost for dense stereo
- Performance degrades in low-texture or low-light conditions, but can be improved via pre-processing

Implementation notes
- Use OpenCV to rectify images and compute disparity.
- Post-process disparity maps to filter noise and compute median disparity inside bounding boxes.

--- 
Algorithm 4: Monocular Depth (Deep Learning e.g., MiDaS)
---
Description
Use a pretrained monocular depth estimator (MiDaS / DPT) to get dense depth prediction from a single image. Combine detection bounding boxes with depth map to estimate object distance (e.g., median depth inside bbox).

Requirements
- Deep model weights and inference runtime (PyTorch)
- Possibly GPU for real-time or near real-time performance

Pros
- Works with single camera
- Robust to occlusion & clutter when model generalizes well
- Can work in varied lighting if model trained for it

Cons
- Heavier compute; may need GPU
- Outputs relative depth (scale ambiguity) — requires scale alignment (e.g., using a known reference or small calibration step)

Implementation notes
- Use a small MiDaS backbone for faster inference (or quantized model).
- Apply a scale alignment step: collect a sample of known distances and compute scale factor between predicted depth and true distances.

--- 
Performance & Practical Recommendations
---
- For strict real-time on CPU-only edge devices: use Pinhole or Bounding-Box methods.
- For best accuracy when hardware allows and a stereo rig exists: use Stereo Vision.
- For single-camera with moderate compute (GPU): Monocular depth with a scale alignment step is powerful.
- Combine methods: e.g., use fast pinhole as primary and MiDaS as fallback or periodic correction for scale drift.

--- 
Integration Plan (suggested)
---
1. Implement all methods as modular functions in `src/distance_methods/`.
2. Provide a `distance_estimation.py` wrapper with a `method` flag to switch algorithms.
3. Add unit tests and sample videos for calibration and fitting.
4. Document instructions for camera calibration and how to obtain focal length in pixels.

--- 
Appendix: Useful equations
---
Pinhole:
  Z = (H_real * f) / h_pixels
Stereo:
  Z = (f * B) / d
where:
  Z = distance (meters)
  H_real = object real-world height (meters)
  f = focal length (pixels)
  h_pixels = bounding box height (pixels)
  B = baseline (meters)
  d = disparity (pixels)

--- 
References
---
- OpenCV documentation (camera calibration, stereo)
- MiDaS: https://github.com/intel-isl/MiDaS
- Pinhole camera model resources
